#!/usr/bin/env python3

import argparse
import os
import datetime
import subprocess

DB = os.path.join(os.path.dirname(__file__), 'db/')

def create_config(config_file, args):
	if os.path.exists(config_file):
		print("\nWARNING: The file "+config_file+" already exists. It will be replaced.\n")
		os.remove(config_file)
		
	with open(config_file, 'w') as conf:
		conf.write("""# This config file was automatically generated by foodme.py ({date})
#Working directory (full path):
workdir: "{workdir}"

# Path to sample sheet:
samples: "{sample_list}"

# Number of threads per sample:
threads_sample: {threads_sample} 
# Max number of threads to use:
threads: {threads}

fastp:
    length_required: {fastp_length}
    qualified_quality_phred: {fastp_min_phred}
    # Options for --cut_tail:
    window_size: {fastp_window}
    mean_quality: {fastp_meanq} 

read_filter:
    min_length: {merge_minlength}
    max_length: {merge_maxlength}
    max_expected_errors: {merge_maxee}
    
cluster:
    cluster_identity: {cluster_id}
    # Path to database for chimera filtering
    chimera_DB: {chimera_db}

blast:
    # Path to the BLAST database
    blast_DB: {blastdb}
    # Path to the folder containing the taxdb database
    taxdb: {taxdb}
    # Report filters
    e_value: {e_val}
    perc_identity: {blast_id}
    qcov: {blast_qcov}
    # Post-filtering
    bit_score_diff: {bitscore}

taxonomy:
    names_dmp: {names_dmp}
    nodes_dmp: {nodes_dmp}
""".format(date=datetime.datetime.now(),
		workdir=args.working_directory,
		sample_list=args.sample_list,
		threads_sample=args.threads_sample,
		threads=args.threads,
		fastp_length=args.fastp_length,
		fastp_min_phred=args.fastp_min_phred,
		fastp_window=args.fastp_window,
		fastp_meanq=args.fastp_meanq,
		merge_minlength=args.merge_minlength,
		merge_maxlength=args.merge_maxlength,
		merge_maxee=args.merge_maxee,
		cluster_id=args.cluster_id,
		chimera_db=args.chimera_db,
		blastdb=args.blastdb,
		taxdb=args.taxdb,
		e_val=args.e_val,
		blast_id=args.blast_id,
		blast_qcov=args.blast_cov,
		bitscore=args.bitscore,
		names_dmp=args.names_dmp,
		nodes_dmp=args.nodes_dmp))
	
def run_snakemake(config_file, args):
	forceall = ("--forceall" if args.forceall else "")
	dryrun = ("-n" if args.dryrun else "")
	conda_prefix= ("--conda-prefix {}".format(args.condaprefix) if args.condaprefix else "")
	call = "snakemake -s {snakefile} --configfile {config_file} --use-conda --cores {cores} {conda_prefix} {forceall} {dryrun}".format(snakefile= args.snakefile,
																																	config_file= config_file,
																																	conda_prefix= conda_prefix,
																																	forceall= forceall,
																																	dryrun = dryrun,
																																	cores=args.threads)
	subprocess.call(call, shell=True)
	
def main(): 
	parser = argparse.ArgumentParser()
	# Path arguments
	parser.add_argument('-l', '--sample_list', required=True, type=os.path.abspath, 
						help="Tab-delimited list of samples and paths to read files. Must contain one line of header, each further line contains sample_name, read1_path, read2_path")
	parser.add_argument('-d', '--working_directory', required=True, type=os.path.abspath,
						help="Directory to create output files")
						
	# Snakemake arguments
	parser.add_argument('--forceall', required=False, default=False, action='store_true',
						help="Force the recalculation of all files")
	parser.add_argument('-n', '--dryrun', required=False, default=False, action='store_true',
						help="Dryrun. Calculate the DAG but do not execute anything")
	parser.add_argument('-t', '--threads', required=False, default=8, type=int,
						help="Maximum number of threads toi use")
	parser.add_argument('--threads_sample', required=False, default=1, type=int,
						help="Number of threads to use per concurent job")
	parser.add_argument('-c', '--condaprefix', required=False, type=os.path.abspath, default=False,
						help="Location of stored conda environment. Allows snakemake to reuse environments.")
	parser.add_argument('-s', '--snakefile', required=False, type=os.path.abspath, default=os.path.join(os.path.dirname(__file__), "Snakefile"), 
						help="Path to the Snkefile in the FOodMe repo")
						
	# Fastp
	parser.add_argument('--fastp_length', required=False, default=50, type=int,
						help="Minimum length of input reads to keep")
	parser.add_argument('--fastp_min_phred', required=False, default=15, type=int,
						help="Minimal quality value per base")
	parser.add_argument('--fastp_window', required=False, default=4, type=int,
						help="Size of the sliding window for tail quality trimming")
	parser.add_argument('--fastp_meanq', required=False, default=20, type=int,
						help="Minimum mean Phred-score in the sliding window for tail quality trimming")					
						
	# Read filter
	parser.add_argument('--merge_minlength', required=False, default=200, type=int,
						help="Minimum length merged reads to keep")
	parser.add_argument('--merge_maxlength', required=False, default=600, type=int,
						help="Maximum length merged reads to keep")
	parser.add_argument('--merge_maxee', required=False, default=1, type=int,
						help="Maximum expected errors in merged reads to keep")
	
	# Cluster
	parser.add_argument('--cluster_id', required=False, default=0.97, type=float,
						help="Minimum identity for clustering sequences in OTUs (between 0 and 1)")
	parser.add_argument('--chimera_db', required=False, type=os.path.abspath,
						default=os.path.join(DB, "blast/mitochondrion.LSU.faa"),
						help="Path to the database for chimera detection. Ideally a database of expected sequences without chimeras.")
	
	# Blast
	parser.add_argument('--blastdb', required=False, type=os.path.abspath,
						default=os.path.join(DB, "blast/mitochondrion.LSU.faa"),
						help="Path to the BLAST database (folder)")
	parser.add_argument('--taxdb', required=False, type=os.path.abspath,
						default=os.path.join(DB, "blast/"),
						help="Path to the BLAST taxonomy database (folder)")
	parser.add_argument('--e_val', required=False, default=1e-30, type=float,
						help="E-value threshold for blast results")
	parser.add_argument('--blast_id', required=False, default=97, type=float,
						help="Minimal identity between the hit and query for blast results (in percent)")
	parser.add_argument('--blast_cov', required=False, default=97, type=float,
						help="Minimal proportion of the query covered by a hit for blast results. A mismatch is still counting as covering (in percent)")
	
	# Taxonomy
	parser.add_argument('--bitscore', required=False, default=128, type=int,
						help="Maximum bit-score difference with the best hit for a blast result to be included in the taxonomy consensus detemination")
	parser.add_argument('--nodes_dmp', required=False, type=os.path.abspath,
						default=os.path.join(DB, "taxdump/nodes.dmp"),
						help="Path to the nodes.dmp file")
	parser.add_argument('--names_dmp', required=False, type=os.path.abspath,
						default=os.path.join(DB, "taxdump/names.dmp"),
						help="Path to the names.dmp file")	
	
	args = parser.parse_args()
	
	# Check constraints
	if args.cluster_id > 1 or args.cluster_id < 0:
		parser.error("'--cluster_id' value must be between 0 and 1")
	if args.blast_id > 100 or args.blast_id < 0:
		parser.error("'--blast_id' value must be between 0 and 100")
	if args.blast_cov > 100 or args.blast_cov < 0:
		parser.error("'--blast_cov' value must be between 0 and 100")
		
	# check files
	for path in [args.chimera_db, args.blastdb, args.taxdb]:
		if not os.path.exists(path):
			print("\nIt seems a database is missing. See Error below.\nYou can create the blast database by using the 'create_blast_db.sh' script.\n")
			raise OSError(2, "No such file or directory" , path)
	
	# Create workdir
	if not os.path.exists(args.working_directory):
		os.makedirs(args.working_directory)
	
	# Create config.yaml
	config_file = os.path.join(args.working_directory, "config.yaml")
	create_config(config_file, args)

	# Execute snakemake
	run_snakemake(config_file, args)
	
	# On quit
	print("\nThank you for using FooDme!\n")

if __name__=='__main__':
		main()