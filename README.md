# FooDMe - A pipeline for Food DNA Metabarcoding

## Description

FooDMe is a pipeline for taxonomic assignement of targeted sequencing reads (DNA Metabarcoding). 
It is meant for food authenticity analysis but could also work for other applications.
It works on raw paired-end Illumina reads, it will trim them, assemble read-pairs and perform a quality filtering, merge samples to filter out chimeras and find OTUs,
map the assembled reads to the OTUs, and finally, perform taxonomic assignment using a nucleotide database. 

FooDMe is built on Snakemake and uses the following tools:

* [Fastp](https://github.com/OpenGene/fastp)
* [VSearch](https://github.com/torognes/vsearch) 
* [BLAST+](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=Download) 
* [Krona](https://github.com/marbl/Krona)

Running FooDMe requires a UNIX environment (tested on Debian GNU/Linux 10 (buster)). An internet connection is required to setup the Databases and Conda environments but is not required to run the pipeline.

## Source

FooDMe is not yet distributed through a centralized hub. It is however Git-versionned.
For the latest version see "Contact".

## Known issues

* When using Blast, the pipeline will crash if no hits are found. 

## Installation

Get the latest version and upack it in the Repo directory:

```bash
REPO=path/to/repo
cd $REPO
tar -xzvf FooDme.tar
```

### Conda environment

FooDMe requires conda to manage environments of all dependencies. You can install conda from [here](https://docs.conda.io/en/latest/miniconda.html) or chose another distribution.
Then you need to setup an environment to lauch the pipeline:

```bash
conda create -n foodme snakemake biopython
```

Additional environments should be installed during the first execution of the pipeline.

### Reference sequences database

A Database of reference sequences should be provided, either in a BLAST or SINTAX compatible format.

### Taxonomy definitions 

The taxdump files nodes.dmp and rankedlineage.dmp are nescessary for various taxonomy operations.
These can be downloaded from the NCBI repository: <https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.tar.gz>

## Usage

### Python wrapper

One can execute the worflow using the provided python wrapper:

```bash
conda activate foodme
python ${REPO}/foodme.py -l /path/to/sample/list -d /working/directory/ 
```

A configuration file will be automatically generated in the working directory with default values. 
These can be overridden by using the following arguments (use `-h` to display help):

```bash
usage: FooDMe [-h] [-v] -l SAMPLE_LIST -d WORKING_DIRECTORY [--forceall] [-n]
              [-T THREADS] [-t THREADS_SAMPLE] [-c CONDAPREFIX] [-s SNAKEFILE]
              [--clean_temp] [--fastp_length FASTP_LENGTH]
              [--fastp_min_phred FASTP_MIN_PHRED]
              [--fastp_window FASTP_WINDOW] [--fastp_meanq FASTP_MEANQ]
              [--merge_minlength MERGE_MINLENGTH]
              [--merge_maxlength MERGE_MAXLENGTH] [--merge_maxee MERGE_MAXEE]
              [--merge_maxns MERGE_MAXNS] [--clustering {distance,abundance}]
              [--cluster_id CLUSTER_ID] [--cluster_minsize CLUSTER_MINSIZE]
              [--chim_denovo] [--chim_ref CHIM_REF] [--tax {blast,sintax}]
              [--nodes_dmp NODES_DMP] [--rankedlineage_dmp NAMES_DMP]
              [--sintaxdb SINTAXDB] [--sintax_cutoff SINTAX_CUTOFF]
              [--blastdb BLASTDB] [--taxdb TAXDB] [--blast_eval BLAST_EVAL]
              [--blast_id BLAST_ID] [--blast_cov BLAST_COV]
              [--bitscore BITSCORE]

```

The size of the output folder can be dramatically reduced (up to 7 times!) by using the `--clean_temp` tag. This will remove large fasta and fastq files generated by the analysis.

### Snakemake

One can also run snakemake directly by providing the path to the snakefile and confiuration file.
For help with snakemake see the [homepage](https://snakemake.readthedocs.io/en/v5.1.4/executable.html).

```bash
conda activate foodme
snakemake -s ${REPO}/Snakefile  --conda-prefix /path/to/conda/envs --configfile /path/to/config.yaml --use-conda
```

Note that by default the pipeline will delete large fastq and fasta files generated during the analysis. To avoid this behavior use the `--notemp` tag

### Sample sheet 

FooDMe execution requires a sample sheet to be provided. This is a tab separated table referencing mate pair files for each samples.
Such a table can be automatically generated with the script `create_sampleSheet.sh`, originally developped by the BfR Study Center for Genome Sequencing and Analysis :

```bash
bash ${REPO}/scripts/create_sampleSheet.sh -f path/to/reads
```

This script includes a range of options for dealing with non-Illumina file name formatting. 

## Reports

FooDMe will generate statistics files (tab-separated text), an html summary report, and many other files that are produced during the analysis.
A typical result folder looks like this: 

```
working_directory/
	.snakemake/ 	Contains the snakemake products, including snakemake log file for troobleshooting
	blast/			Contains files with Blast report, filtering of blast results and taxonomy determination
	logs/			Contains the log files for troobleshooting specific steps
	reports/		Contains aggregated quality statistics of the different steps of the analysis and the html report
	Sample_1/		Sample-level quality statistics and merged-reads and quality filtered fasta files
	trimmed/		Contains the trimmed fastq files and trimming reports
	VSEARCH/		Contains aggregated fasta files and cluster files as well as chimeras
	config.yaml		Will be generated here by the python wrapper
```

## Workflow details

### Read trimming, merging, and quality filtering

Raw reads are pre-processed with fastp to remove adapters and mask low quality regions. 
Mate pairs are then merged and the assembled reads are filtered for the their length and overall quality.

### Clustering and chimera filtering

Reads from all samples are pooled and dereplicated (exact match). Reads are then clustered in centroids using VSearch's `cluster_size` option. 
By default, clustering is performed using a Distance-greedy clustering algorithm but an abundance-greedy clustering can also be performed (the 16 top sequences will be considered).
Clusters consisting of a single read are removed from the analysis.
Chimeras are then optionnaly detected and removed using *de novo* and database-based approaches.

### Taxonomic assignment

Dereplicated reads are mapped to the OTUs using VSearch global alignment option `usearch_global`. 
Taxonomic assignment can be performed either using Blast or Sintax.

Taxonomy granularity is as follow: 

* Species
* Genus
* Family
* Order
* Class
* Phylum
* Kingdom

#### BLAST

Each OTU is blasted against a nucleotide reference database to determine the Taxa of origin. 
Hits are filtered by bit-score difference to the bast hit to remove lower similarity hits. 
A taxonomic consensus is then determined as the lowest common node of all hits.

#### Sintax

For each OTU the top k-mer hits are found in a database and a bootstrap support value is reported for each taxonomic rank.
Taxonomic consensus is determined as the lowest rank with a bootstrap value above threshold.

## Credits

This pipeline includes thrird-party source-code, available under BSD-3 license:

 * The `create_sampleSheet.sh` script as well as some code snippets were written by Carlus Deneke (Bundesinstitut für Risikobewertung). Source: [AQUAMIS](https://gitlab.com/bfr_bioinformatics/AQUAMIS).
 * Taxonomic lineage extraction uses the `ncbi_taxdump_utils.py` module (with minimal modifications) from Titus Brown (University of California). Source: [2018-ncbi-lineages](https://github.com/dib-lab/2018-ncbi-lineages>).

## Contact

For questions about the pipeline, problems, suggestions or requests, feel free to contact:

Grégoire Denay, Chemisches- und Veterinär-Untersuchungsamt Rhein-Ruhr-Wupper 

<gregoire.denay@cvua-rrw.de>
