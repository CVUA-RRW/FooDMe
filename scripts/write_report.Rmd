---
title: "FooDMe - Food DNA Metabarcoding - analysis report"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
theme: default
---

<style type="text/css">
.main-container {
  max-width: 1800px;
  margin-left: auto;
  margin-right: auto;
}
</style>

```{r setup, include=FALSE}

knitr::opts_chunk$set(out.width = '80%',fig.asp= 0.5,fig.align='center',echo=FALSE, warning=FALSE, message=FALSE)
options(markdown.HTML.header = system.file("misc", "datatables.html", package = "knitr"))

library(DT, quietly = T)
library(tidyverse, quietly = T)
library(plotly, quietly=T)

executor <- Sys.info()["user"]
```

---
author: `r paste0(executor)`
---

```{r init}
workdir <- snakemake@params[["workdir"]]
overview <- snakemake@input[["summary"]]
fastp <- snakemake@input[["fastp"]]
qc_filtering <- snakemake@input[["qc_filtering"]]
clustering <- snakemake@input[["clustering"]]
mapping <- snakemake@input[["mapping"]]
blast <- snakemake@input[["blast"]]
blast_rep <- snakemake@input[["blast_rep"]]
taxonomy <- snakemake@input[["taxonomy"]]
result <- snakemake@input[["result"]]
db <- snakemake@input[["db"]]
soft <- snakemake@input[["soft"]]
merged_qc <- snakemake@input[["merged_qc"]]

# infer run name from workdir
run <- head(tail(strsplit(workdir,"/")[[1]],2),1)

# Number of samples
nsamples <- nrow(read.csv(file = overview, sep = "\t", check.names=FALSE))
```

# Analysis report for run: `r run` {.tabset}

## Overview

* Run name: `r head(tail(strsplit(workdir,"/",fixed = T)[[1]],3),1)`
	
* Number of samples: `r nsamples`

* Run directory: `r workdir`

* config file: <a href=../config.yaml>`r file.path(workdir,"config.yaml")`<a/>

* log files in folder: `r file.path(workdir,"logs")`

* snakemake logs in folder: `r file.path(workdir,".snakemake")`

## Quality summary

```{r summary}
data_table <- read.csv(file = overview, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Trimming statistics

Click on 'file' in the links column to access the full fastp html report.

```{r trimming}
data_table <- read.csv(file = fastp, sep = "\t", check.names=FALSE)

# Create hyperlinks
data_table$links <- paste0("<a href=", data_table$link_to_report, ">file</a>")
data_table$link_to_report = NULL

datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Read filtering statistics

Merged reads were filtered with a length range of `r snakemake@params[["min_len"]]` to `r snakemake@params[["max_len"]]` nucleotides and `r snakemake@params[["max_ee"]]` maximum cumulated expected errors.

```{r filtering}
data_table <- read.csv(file = qc_filtering, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Clustering statistics

```{r clustering}
data_table <- read.csv(file = clustering, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Mapping statistics

```{r mapping}
data_table <- read.csv(file = mapping, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Blast report

```{r blast_rep}
data_table <- read.csv(file = blast_rep, sep = "\t", header= FALSE, check.names=FALSE,
						col.names = c("OTU", "Target sequence ID", "e-value", "Percent identity", "Bit-score", "Accession number", "Taxid", "Scientific name", "Common name", "Sequence description")
						) %>% separate("OTU", c("Query", "Count"), sep=";size=")
						
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```
 
## Blast filtering

```{r blast}
data_table <- read.csv(file = blast, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Taxonomic assignment statistics

```{r taxonomy}
data_table <- read.csv(file = taxonomy, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Metabarcoding results

```{r results}
data_table <- read.csv(file = result, sep = "\t", check.names=FALSE)
datatable(data_table, filter = 'top', rownames = FALSE, escape = FALSE,
		extensions = list("ColReorder" = NULL, "Buttons" = NULL),
		options = list(	
					dom = 'BRrltpi',
					autoWidth=FALSE,
					scrollX = TRUE,
					lengthMenu = list(c(10, 50, -1), c('10', '50', 'All')),
					ColReorder = TRUE,
					buttons =
					list(
						'copy',
						'print',
						list(
						extend = 'collection',
						buttons = c('csv', 'excel', 'pdf'),
						text = 'Download'
						),
						I('colvis')
						)
						))
```

## Quality plots

```{r qcplots_import}

# import for QC
qc_table <- data.frame()
for (file in merged_qc){
	data <- read.csv(file = file, sep="\t", check.names=FALSE) %>%
			mutate(count =  Recs - lead(Recs, default= 0)) %>%
			add_column(sample = str_split(file, "/", n=2)[[1]][1], .before = 1)	%>%
			select(sample, Pos, count, Min_EE, Low_EE, Med_EE, Hi_EE, Max_EE)
	qc_table <- rbind(qc_table, data)
	}

ee_table <- qc_table %>% group_by(sample) %>% top_n(1, Pos)

# Import for Read number
read_table <- read.csv(file = qc_filtering, sep = "\t", check.names=FALSE) %>%
			select(Sample, `Quality filtered reads`)

```

### Read number

```{r qcplot_reads}

p <- ggplot(data= read_table, aes(x=Sample, y=`Quality filtered reads`)) +
		geom_bar(stat= "identity", fill="gray", color="black") +
		geom_text(aes(label=`Quality filtered reads`), vjust=-0.3, size=3.5) +
		theme_minimal()
p

```

### Read length distribution

```{r qcplot_length}

qc_table <- qc_table %>% group_by(sample) %>%
			mutate(weights = count/sum(count))
p <- ggplot(qc_table, aes(x=sample, y= Pos, weight= weights))+
		geom_violin(fill="gray") +
		labs(x="Sample", y= "Merged read length") +
		geom_hline(yintercept=snakemake@params[["max_len"]], linetype="dashed", color = "red") +
		geom_hline(yintercept=snakemake@params[["min_len"]], linetype="dashed", color = "red") +
		theme_minimal()
p

```

### Read quality distribution

```{r qcplot_ee}

p <- ggplot(ee_table, aes(x=sample, ymin= Min_EE, lower = Low_EE, middle= Med_EE, upper= Hi_EE, ymax= Max_EE)) +	
		geom_boxplot(stat= "identity", fill = "gray") +
		labs(x="Sample", y= "Expected errors per read") +
		geom_hline(yintercept=snakemake@params[["max_ee"]], linetype="dashed", color = "red") +
		theme_minimal()
p

```

Red dotted lines represent the cutoff values for filtering.

## Versions

### Databases

```{r db_versions}
db_table <- read.csv(file = db, sep = "\t", check.names=FALSE)
knitr::kable(db_table)
```

### Softwares

```{r soft_versions}
soft_table <- read.csv(file = soft, sep = "\t", check.names=FALSE)
knitr::kable(soft_table)
```

## Help

```{r help}
keyword <- c(
	"Q30 rate",
	"Filtered reads",
	"Mapped reads",
	"OTU",
	"Consensus",
	"Duplication rate",
	"Insert size peak",
	"Merged Reads",
	"Unique sequence",
	"Reads kept",
	"Centroid",
	"Singleton centroid",
	"Chimera",
	"Count",
	"Blast hit",
	"Bit-score")
	
definition <- c(
	"Fraction of bases with Q30 or higher.",
	"Reads selected after quality filtering (merged reads).",
	"Reads successfully mapped to an OTU (merged reads).",
	"Cluster of highly similar sequences (Operational Taxonomic Unit).",
	"Taxonomic agreement between Blast hits (last common ancestor).",
	"Proportion of duplicated reads.",
	"Read length value for the most reads.",
	"Reassembled forward and reverse reads.",
	"Unique nucleotide sequence in the dataset. Merged reads that are the exact copy of each others are pooled together.",
	"Proportion of raw reads that were kept through the merging and quality filtering steps.",
	"Representative sequence of a cluster.",
	"Centroid containing a single read.",
	"Chimeric reads generated through PCR or sequencing artifacts.",
	"Number of reads.",
	"Positive alignment match between the query sequence (OTU) and the database.",
	"Unbiased measure of sequence similarity. Does not depend on the database size.")

help_tab <- data.frame(Keyword=keyword, Definition=definition)

knitr::kable(help_tab)
```