{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":""},{"location":"#welcome-to-the-documentation-of-foodme","title":"Welcome to the documentation of FooDMe","text":"<p>Support for FooDMe has ended ! Check out FooDMe2</p> <p>FooDMe2 is a complete rewrite of FooDMe. We designed FooDMe 2 to be more flexible and take away some of the complexity encountered in FooDMe 1. This not only concerns the installation procedure, which is vastly streamlined now, but also the process of configuring and starting individual analysis runs. The new implementation also maes it easier to deploy, maintain, and to add additonal functionalities in the future.</p>"},{"location":"#overview","title":"Overview","text":"<p>FooDMe processes paired-end Illumina reads in the following way:</p> <ul> <li>Primer trimming</li> <li>Quality filtering</li> <li>Sequence clustering based on identity clustering (OTUs), dereplication, or denoising (ASVs)</li> <li>Similarity-search cluster sequences in a user-provided database</li> <li>Determine a taxonomic consensus for each cluster sequence</li> <li>Outputs quality reports and results in human-readable formats</li> </ul>"},{"location":"#quick-links","title":"Quick links","text":"<ul> <li>First time? Check the User Guide.</li> <li>For a history of the releases see Changelog.</li> <li>A list of Frequently Asked Questions.</li> <li>Get help here!</li> </ul>"},{"location":"#citation","title":"Citation","text":"<p>If you use FooDMe for research please cite us!</p> <p>The source code is deposited on Zenodo and citable using the DOI at the top of this page (Last release)</p> <p>We also published a benchmarking study for 16S Metabarcoding of meat products:</p> <p>Denay, G.; Preckel, L.; Petersen, H.; Pietsch, K.; W\u00f6hlke, A.; Br\u00fcnen-Nieweler, C.  Benchmarking and Validation of a Bioinformatics Workflow for Meat Species Identification Using 16S rDNA Metabarcoding.  Foods 2023, 12, 968. https://doi.org/10.3390/foods12050968 </p>"},{"location":"#connex-tools","title":"Connex tools","text":"<p>BaRCoD: A snakemake workflow aiming at recovering and analyzing barcodes for metabarcoding experiments. Using a set of primers, finds possible amplicon in the database (or a taxonomic subset thereof)  and performs pairwise alignements of barcodes. Available from Github.</p> <p>TaxidTools: A Python library to load, process and work with taxonomy definitions. Documentation on the homepage.</p>"},{"location":"#about-us","title":"About us","text":"<p>FooDMe is developed by Gr\u00e9goire Denay at the Chemical and Veterinary Investigation Office Rhein-Ruhr-Wupper,  an official public laboratory in the field of consumer health protection.</p> <p>More about CVUA-RRW.</p>"},{"location":"faqs/","title":"Frequently Asked Questions","text":"<p>Q: Do I need to install mamba?</p> <p>A: Installing mamba is optional. Simply using conda should be enough, although considerably slower, to  create environments. If using only conda you may need to add the argument <code>--conda-frontend conda</code> when executing snakemake.</p> <p>Q: Can I use FooDMe for single end reads?</p> <p>A: FooDMe is designed for Illumina paired-end sequencing and single read sequencing is not supported.</p> <p>Q: Are IonTorrent data supported?</p> <p>A: Not yet. But it should be possible with some modifications. Get in touch if this is something you need.</p> <p>Q: Can I use FooDMe for bacteria metabarcoding?</p> <p>A: You could but it is not recommended. There are pipeline more suitables for micro-organisms out there.</p> <p>Q: Can I use nucleotide databases other than RefSeq or BLAST NT (e.g. BOLD)?</p> <p>A: Any nucleotide database can be used, however they need to be formatted by the BLAST+ tools suit to be usable in FooDMe. Consult the BLAST documentation for instructions.</p> <p>Q: In the reports, some decimals are point-separated and others are comma-separated. Why is that and how do I fix it?</p> <p>A: The comma-spearated values are generated in some cases depending on your system's langugae (e.g. French, German,...). To enforce point-separated values, change your system locale by modifying the <code>~/.bashrc</code> file with <code>LANG=C</code>. </p>"},{"location":"help/","title":"Getting help","text":""},{"location":"help/#questions","title":"Questions","text":"<p>Still confused?  Check the FAQs and the open questions. If you don\u00b4t find what you need there, then ask a new question!</p>"},{"location":"help/#bug-report","title":"Bug report","text":"<p>Is something not working like it should? Help us make FooDMe better and submit a bug report.</p>"},{"location":"help/#new-features","title":"New features","text":"<p>If you have ideas to improve the workflow, share it with us! </p>"},{"location":"about/changelog/","title":"Release notes","text":""},{"location":"about/changelog/#172","title":"1.7.2","text":"<p>Fixing fix</p>"},{"location":"about/changelog/#171","title":"1.7.1","text":""},{"location":"about/changelog/#bugfix","title":"Bugfix","text":"<p>Fixing some issues with blast report generation when using newer BLAST databases. Source of the problemis still unclear but appears to be linked to new realeses of the BLAST db and concerns only specific taxids or sequences.</p>"},{"location":"about/changelog/#170","title":"1.7.0","text":""},{"location":"about/changelog/#breaking-changes","title":"Breaking changes","text":"<p>Older configuration files are not compatible anymore. Update the files by adding the following line should yield the same results as before:</p> <pre><code>blast_filter_low_complexity: True\n</code></pre>"},{"location":"about/changelog/#new-features","title":"New features","text":"<p>It is now possible to desactivate the defualt low-complexity filter of the BLAST search. This can be advantageous if you expect your barcode to contain low-complexity sequences which could  prevent getting any match at all. This behaviour can be activated/deactivated by changing the <code>blast_filter_low_complexity</code> from <code>True</code> to <code>False</code>.</p> <p>The default behaviour (<code>False</code>) uses the default 'DUST' filter of the blast tool: <code>-dust 20 64 1 -soft_masking true</code>.</p>"},{"location":"about/changelog/#166","title":"1.6.6","text":""},{"location":"about/changelog/#fixes","title":"Fixes","text":"<ul> <li>Corrects parsing of the <code>trim_primers_3end</code> parameter (#64)</li> <li>Added a primer disambiguation step that converts primers sequences in the IUPAC ambiguous nucleotide format to their explicit forms (#63)</li> </ul>"},{"location":"about/changelog/#165","title":"1.6.5","text":""},{"location":"about/changelog/#fixes_1","title":"Fixes","text":"<ul> <li>BLAST rule now correctly uses the <code>threads_sample</code> parameter instead of <code>threads</code>. This results in better ressource management for the BLAST rule.</li> </ul>"},{"location":"about/changelog/#164","title":"1.6.4","text":"<p>Benchmarking paper for 16S Metabarcoding of meat products is online:</p> <p>Denay, G.; Preckel, L.; Petersen, H.; Pietsch, K.; W\u00f6hlke, A.; Br\u00fcnen-Nieweler, C.  Benchmarking and Validation of a Bioinformatics Workflow for Meat Species Identification Using 16S rDNA Metabarcoding.  Foods 2023, 12, 968. https://doi.org/10.3390/foods12050968 </p>"},{"location":"about/changelog/#new-features_1","title":"New features","text":"<ul> <li>It is now possible to filter specific sequences form the database using a list of accession provided in a text file using the <code>seq_blocklist</code> parameter (#60).</li> </ul>"},{"location":"about/changelog/#improvements","title":"Improvements","text":"<ul> <li>Taxa names are now displayed in the benchmarking report</li> <li>Fixed many typos and errors in the documentation</li> <li>Improved report aesthetics</li> <li>Improved the fetch_nt_blast.sh script to make it easier to resume interupted processes, also more robust</li> </ul>"},{"location":"about/changelog/#fixes_2","title":"Fixes","text":"<ul> <li>Fixed confusion matrix calculation when the expected taxid rank is above the target rank</li> </ul>"},{"location":"about/changelog/#163","title":"1.6.3","text":""},{"location":"about/changelog/#improvements_1","title":"Improvements","text":"<ul> <li>Benchmarking: confusion table now reports prediction rank as well</li> </ul>"},{"location":"about/changelog/#bug-fix","title":"Bug fix","text":"<ul> <li>Fixed major problem in confusion matrix determnination for the benchmark module. Prior to this fix, False negatives were not correctly reported.</li> <li>Small fix to report aggregation rules for a rarely happening failure</li> <li>Fixed composition summary when input is empty</li> </ul>"},{"location":"about/changelog/#162","title":"1.6.2","text":""},{"location":"about/changelog/#bug-fix_1","title":"Bug fix","text":"<ul> <li>Moved the benchmark module form a rule to a workflow. THis allows to ignore the benchmark arguments when running routine   analysis, as originally intended. Benchmarking is now called with the  <code>-s path/to/FooDMe/woorkflow/benchmark</code> argument.   This has no impact on the basic analysis (no <code>-s</code> argument) or the paramspace analysis (<code>-s path/to/FooDMe/woorkflow/paramspace</code>).</li> </ul>"},{"location":"about/changelog/#documentation","title":"Documentation","text":"<ul> <li>Fixed a few errors in the <code>config.yaml</code> comments</li> <li>Modified documentation of the <code>benchmark</code> module</li> </ul>"},{"location":"about/changelog/#161","title":"1.6.1","text":""},{"location":"about/changelog/#bug-fix_2","title":"Bug fix","text":"<ul> <li>Fixed handling of last common node calculation in confusion matrix where last common ancesotr is the root node.</li> <li>Modified dependencies in environement definition files to solve some issues in conda solving</li> </ul>"},{"location":"about/changelog/#160","title":"1.6.0","text":"<p>This update is not backwards compatible. A configuration file update is nescessary.</p>"},{"location":"about/changelog/#new-features_2","title":"New features","text":"<ul> <li>Added a new Snakefile for parameter space exploration.   Basically acts as a wrapper around the foodme benchmark workflow    for parameter grid search using snakemake's <code>Paramspcae</code> utility.</li> </ul>"},{"location":"about/changelog/#non-backward-compatible-changes","title":"Non backward compatible changes","text":"<ul> <li>Flattened the parameter structure in the configuration.   This is more compatible with the <code>--config</code> CLI argument and   was required for the implementation of the parameter space exploration workflow.   This requires users to update their configurations.</li> </ul>"},{"location":"about/changelog/#documentation_1","title":"Documentation","text":"<ul> <li>Added documentation for the <code>paramspace</code> workflow.</li> </ul>"},{"location":"about/changelog/#151","title":"1.5.1","text":""},{"location":"about/changelog/#bug-fix_3","title":"Bug fix","text":"<ul> <li>Added missing parameters to meat config file</li> <li>Fix dtype parsing in confusion matrix calculation</li> <li>Fix package version reporting</li> <li>Fix Error calculation in benchmarking module</li> <li>Fix multiple plotting in benchmarking report</li> </ul>"},{"location":"about/changelog/#150","title":"1.5.0","text":"<p>This update is not backwards compatible. A configuration file update is nescessary.</p>"},{"location":"about/changelog/#new-features_3","title":"New features","text":"<ul> <li>Benchmark module is live with possibility to compare results to an expected sample   composition. The benchmark module will output the comparison results and several useful    metrics in an HTML report. It can be used directly for validation or parameter space exploration.</li> </ul>"},{"location":"about/changelog/#non-backward-compatible-changes_1","title":"Non backward compatible changes","text":"<ul> <li>Added the benchmarking module which can be called with <code>snakemake benchmark</code></li> <li>Added required parameters in the config file</li> <li>The python wrapper is now deprecated. See the documentation on how to use configuration files.</li> </ul>"},{"location":"about/changelog/#dependencies","title":"Dependencies","text":"<ul> <li>Added a dependency to Scikit-learn</li> <li>Updated R packages dependency in the <code>rmarkdown</code> environment</li> </ul>"},{"location":"about/changelog/#documentation_2","title":"Documentation","text":"<ul> <li>Added documentation for the benchmark module</li> </ul>"},{"location":"about/changelog/#149","title":"1.4.9","text":""},{"location":"about/changelog/#bug-fix_4","title":"Bug fix","text":"<ul> <li>Added missing <code>pandas</code> dependency in <code>taxidTools</code> environment</li> <li>Moved log directive to top of python scripts to catch import errors</li> <li>Replaced all <code>bc</code> callsby <code>printf</code> statements in <code>vsearch.smk</code> (#52)</li> <li>Improved logging for OTU workflow</li> <li>Added test suite for OTU workflow</li> </ul>"},{"location":"about/changelog/#documentation_3","title":"Documentation","text":"<ul> <li>Improved Conda installation guide by quoting the Bioconda guide and adding new snakemake requirement to set sstrict channel priority</li> </ul>"},{"location":"about/changelog/#148","title":"1.4.8","text":""},{"location":"about/changelog/#bug-fix_5","title":"Bug fix","text":"<ul> <li>Fixed header in <code>consensus-table.tsv</code></li> <li>Fixed a bash synthax misuse in the calculation of VSearch statistics</li> </ul>"},{"location":"about/changelog/#documentation_4","title":"Documentation","text":"<ul> <li>Moved the documentation to the homepage at https://cvua-rrw.github.io/FooDMe/</li> </ul>"},{"location":"about/changelog/#147","title":"1.4.7","text":""},{"location":"about/changelog/#bug-fix_6","title":"Bug fix","text":"<ul> <li>Fixed missing report-wise reports</li> </ul>"},{"location":"about/changelog/#146","title":"1.4.6","text":""},{"location":"about/changelog/#bug-fix_7","title":"Bug fix","text":"<ul> <li>The parameter <code>taxid_filter</code> now only accepts integers, default config values have been changed (#42).</li> </ul>"},{"location":"about/changelog/#improvements_2","title":"Improvements","text":"<ul> <li>Now correctly reports composition as both percentage of total usable reads and assigned reads (#41) </li> <li>Added a configuration file for 16S birds and mammals experiments</li> </ul>"},{"location":"about/changelog/#145","title":"1.4.5","text":""},{"location":"about/changelog/#documentation_5","title":"Documentation","text":"<ul> <li>The usage of the python warpper is not recommended. Prefer the use a yaml configuration file. </li> <li>Pending deprecation warning added to the python wrapper</li> <li>Expanded documentation on the use of the config file </li> </ul>"},{"location":"about/changelog/#improvements_3","title":"Improvements","text":"<ul> <li>Improved error handling and logging for the DADA2 steps. Will now correctly output number of reads and denoisin/merging results for failing samples.</li> </ul>"},{"location":"about/changelog/#144","title":"1.4.4","text":""},{"location":"about/changelog/#improvements_4","title":"Improvements","text":"<ul> <li>Now unpacks trimmed read files on a sample wise fashion prior to Dada2 denoising instead of unpacking all samples at once. This should reduce the memory use during the analysis.</li> <li>Preventively fixed a pandas CopyWarning (#31) and FutureWarning</li> <li>Updated dependencies to newer versions. NCBI's upcoming new identifier definitions should be supported (#33).</li> <li>Check compatibility with snakemake v7 (#34)</li> <li>Dependency taxidTools now handled through conda environment and therefore not needed in the base environment anymore (#36). </li> <li>Reorganised logging (#27)</li> <li>Fully linted and formatted (#28)</li> </ul>"},{"location":"about/changelog/#143","title":"1.4.3","text":""},{"location":"about/changelog/#bug-fix_8","title":"Bug Fix","text":"<ul> <li>Fixed a variable refernece breaking Vsearch pipeline</li> <li>Fixed time display upon pipeline completion on success or error</li> </ul>"},{"location":"about/changelog/#142","title":"1.4.2","text":""},{"location":"about/changelog/#bug-fix_9","title":"Bug Fix","text":"<ul> <li>Fixed wrapper</li> </ul>"},{"location":"about/changelog/#141","title":"1.4.1","text":""},{"location":"about/changelog/#improvements_5","title":"Improvements:","text":"<ul> <li>Moderate performance improvements due to saving taxonomy as a filtered JSON file. Expect the workflow to be about 1 min faster per sample.</li> </ul>"},{"location":"about/changelog/#bug-fixes","title":"Bug fixes:","text":"<ul> <li>Fixed Github version paring for lightweight tags. </li> </ul>"},{"location":"about/changelog/#standardization","title":"Standardization","text":"<ul> <li><code>tests</code> was renamed <code>.tests</code></li> <li>Linting and reorganize workflow to match be closer to snakemake standards</li> <li>Added JSON-Schema validation for the config and sample sheet files</li> <li>Added the possibility to export a Snakemake report containing QC summaries and results as well as the workflow runtime and DAG using the <code>--report</code> argument (snakemake CLI only)</li> </ul>"},{"location":"about/changelog/#version-140","title":"Version 1.4.0","text":""},{"location":"about/changelog/#implementation-changes","title":"Implementation changes:","text":"<ul> <li>Migrated to TaxidTools version 2. The taxidTools package must now be installed via conda or pip before starting th epipeline (See README.md).</li> <li>Modified default parameters of the config file and python laucher with more sensible values</li> </ul>"},{"location":"about/changelog/#improvements_6","title":"Improvements:","text":"<ul> <li>Expand disambiguation info with the frequency of each species (#17)</li> <li>Add minimum consensus filter as an alternative to last common ancestor. Use it with the parameter <code>--min_consensus</code>. The value be be in the interval (0.5;1], 1 being a last common ancestro behavior and 0.51 a simple majority vote.</li> <li>Added blocklist of taxids to mask (#13). Default behaviour is to mask extinct taxids. Users can skip this steps or provide their own blocklist with the <code>--blocklist</code> parameter.</li> </ul>"},{"location":"about/changelog/#bug-fixes_1","title":"Bug fixes","text":"<ul> <li>Cluster that do not find a matching reference in BLAST are not counted towards the compoisiton total anymore. Additionnaly the number of assigned reads is now visible in the summary report(#12)</li> <li>Fixed the calculation of the \"No primer found\" field under the triming statistics (#19)</li> </ul>"},{"location":"about/changelog/#version-135","title":"Version 1.3.5","text":"<ul> <li>Upgraded Dada2 dependency to version 1.20</li> </ul>"},{"location":"about/changelog/#version-134","title":"Version 1.3.4","text":"<ul> <li>Upgraded dependencies to last (conda) version</li> </ul>"},{"location":"about/changelog/#version-133","title":"Version 1.3.3","text":"<ul> <li>Test now runs with just <code>snakemake --cores 1 --use-conda</code></li> <li>Added CI in github actions</li> <li>Reworked environments definition files, environments should build correctly.</li> </ul>"},{"location":"about/changelog/#version-132","title":"Version 1.3.2","text":"<ul> <li>Added a very basic test script. This is meant to test the installation - not provide unit testing</li> <li>Added an example of expected output</li> <li>Fixed Snakemake version</li> <li>Fixed summary report</li> </ul>"},{"location":"about/changelog/#version-131","title":"Version 1.3.1","text":"<ul> <li>Workflow will no longer crash on blank samples</li> <li>Now reports the proportion of reads discarded during primer trimming</li> </ul>"},{"location":"about/changelog/#version-130","title":"Version 1.3.0","text":""},{"location":"about/changelog/#incompatible-changes","title":"Incompatible changes","text":"<ul> <li>Now requires user to provide a fasta file with primer sequences</li> </ul>"},{"location":"about/changelog/#new-features_4","title":"New features","text":"<ul> <li>Taxonomic reports now include a 'disambiguation' field summarizing the different blast hit for each cluster</li> <li>Primers will now be trimmed for the reads before quality trimming. It is possible to trim primers on both ends</li> </ul>"},{"location":"about/changelog/#minor-changes","title":"Minor changes","text":"<ul> <li>Performance fix for the display of large tables in the html report</li> </ul>"},{"location":"about/changelog/#version-122","title":"Version 1.2.2:","text":"<ul> <li>Updated BLAST+ and Fastp to the latest version</li> <li>Report now includes links to BLast reports</li> <li>Blast report now includes number of mismatch, gaps and alignment length</li> <li>Added the --skip_adapter_trimming option to disable adapter trimming in fastp (only recommended for artificial dataset)</li> </ul>"},{"location":"about/changelog/#version-121","title":"Version 1.2.1:","text":"<ul> <li>taxidTools is now a submodule</li> <li>Cloning the repository should now be done with '--recurse-submodules'</li> <li>taxidtools updated to version 2</li> <li>Adapted scripts to the new version of taxidtools</li> <li>Changed BLAST database masking to not be silent about taxids missing from the Taxdump definition files</li> </ul>"},{"location":"about/changelog/#version-120","title":"Version 1.2.0:","text":""},{"location":"about/changelog/#new-features_5","title":"New features","text":"<ul> <li>Added the option to filter the BLAST search by taxid</li> </ul>"},{"location":"about/changelog/#fixes_3","title":"Fixes","text":"<ul> <li>Fixed a performance issue for BLAST filtering</li> <li>Better error handling for LCA determination</li> <li>Snakemake logging has been moved to the logs folder</li> </ul>"},{"location":"about/changelog/#version-110","title":"Version 1.1.0:","text":""},{"location":"about/changelog/#new-features_6","title":"New features","text":"<ul> <li>Added primer trimming option (experimental)</li> <li>Added subspecies to taxonomy levels</li> <li>Added a helper script to fetch the BLAST nt database</li> </ul>"},{"location":"about/changelog/#fixes_4","title":"Fixes","text":"<ul> <li>Fixed Krona broken link in report</li> <li>Fixed BLAST filtering for floating point values of bitscores</li> <li>Fixed crash upon absence of BLAST hits</li> <li>Fixed BLAST database version reporting</li> </ul>"},{"location":"about/changelog/#version-100","title":"Version 1.0.0:","text":"<ul> <li>initial release</li> </ul>"},{"location":"about/contributing/","title":"Contribution guide","text":"<p>Maintaining a repository is a lot of work and it is only possible with the input for its users. That's why we love to get your feedback and continuously improve! We want to contibuting to this project as easy and transparent as possible. That's why we ask all contributions to go through a GitHub workflow.</p>"},{"location":"about/contributing/#ask-a-question-report-a-bug-or-suggest-new-features","title":"Ask a question, report a bug, or suggest new features","text":"<p>Wether you wish to report a bug, discuss the current state of the code, how to use it, or propose new features, the GitHub repository is the place to start.</p> <p>Go to 'Issues' in the repository's menu and use the search bar to look for your issue/question, maybe the discussion already exists! if you don't find what you are looking for, use the green button 'New Issue' and select the correct template (Question, Bug report or New feature).</p>"},{"location":"about/contributing/#bug-reports","title":"Bug reports","text":"<p>Solving Bugs is not always easy and usually requires to be able to precisely understand what happened. Try to include the following in your report, so we can solve the problems quickly:</p> <ul> <li>A quick summary and/or background</li> <li>The software versions</li> <li>A precise description of the problem</li> <li>Join input data that reproduce the problem if possible</li> <li>Attach relevant log files to the issue</li> </ul>"},{"location":"about/contributing/#improving-the-documentation","title":"Improving the documentation","text":"<p>Writting a documentation is a big task and we are gratefull for any help to expand and improve it.</p> <p>If you find a specific section of the documentaiton unclear, or would like to see it in other languages, we would love to hear about it, or if you feel like it, try suggesting modifications.</p>"},{"location":"about/contributing/#writting-tests","title":"Writting tests","text":"<p>The critical parts of the workflow are currently verified by unit tests. These could always be improved and/or expanded. If you like to write tests, check the <code>.tests/unit</code> folder and suggest changes.</p>"},{"location":"about/contributing/#submitting-changes","title":"Submitting changes","text":"<p>All changes in the repository are made through pull requests.</p>"},{"location":"about/contributing/#github-worflow","title":"Github worflow","text":"<p>To submitt a pull request:</p> <ul> <li>Fork the repository and create your branch from <code>main</code></li> <li>Make your changes to the code base/documentation/tests</li> <li>If you have added code that shoul dbe testes, add tests</li> <li>If you have changed the parameters or formating of the inputs, update the documentation</li> <li>Ensure that your code passes the tests and lints, this will be automatically   tested when you submit your pull request</li> <li>Issue your pull request and wait for a maintainer to review the changes</li> </ul> <p>Please provide precise and sufficient information on the changes you performed when you submit your pull request.</p>"},{"location":"about/contributing/#tests","title":"Tests","text":"<p>It is a good idea to locally run tests before submitting your pull request.</p>"},{"location":"about/contributing/#unit-tests","title":"Unit tests","text":"<p>We use pytest for running unit tests. Make sure your current environment supports a recent version of python (3.9 or 3.10).</p> <pre><code>python -m pip install --upgrade pip\npip install pytest\npip install -r .tests/unit/requirements.txt  # Install tests dependencies\npytest .tests/unit\n</code></pre>"},{"location":"about/contributing/#integration-test","title":"Integration test","text":"<p>It is always a good idea to verify that the workflow runs properly as a whole. For this activate a conda enviroment with a recent version of snakemake (see the documentation) and run the following test:</p> <pre><code>snakemake --cores 1 --use-conda --configfile .tests/integration/config/config.yaml\n</code></pre>"},{"location":"about/contributing/#license","title":"License","text":"<p>By submitting changes to this repository, you agree that your contributions will be licensed under its BSD-3 Clauses License.</p>"},{"location":"about/license/","title":"License","text":"<p>Copyright 2020-2023, Gregoire Denay.</p> <p>Redistribution and use in source and binary forms, with or without  modification, are permitted provided that the following conditions are met:</p> <ol> <li> <p>Redistributions of source code must retain the above copyright notice, this  list of conditions and the following disclaimer.</p> </li> <li> <p>Redistributions in binary form must reproduce the above copyright notice,  this list of conditions and the following disclaimer in the documentation  and/or other materials provided with the distribution.</p> </li> <li> <p>Neither the name of the copyright holder nor the names of its contributors  may be used to endorse or promote products derived from this software without  specific prior written permission.</p> </li> </ol> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE  ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE  LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)  ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</p>"},{"location":"about/references_links/","title":"References and external links","text":"<p>This pipeline uses several tools and external ressources.  Documentation for these can be found at the links below:</p> Tool Link BLAST https://blast.ncbi.nlm.nih.gov/Blast.cgi Conda https://docs.conda.io/en/latest/ Cutadapt https://cutadapt.readthedocs.io/en/stable/ create_sampleSheet https://gitlab.com/bfr_bioinformatics/AQUAMIS DADA2 https://benjjneb.github.io/dada2/ DT https://rstudio.github.io/DT/ fastp https://github.com/OpenGene/fastp Krona https://bio.tools/krona Pandas https://pandas.pydata.org/ RMarkdown https://rmarkdown.rstudio.com/ SeqTK https://github.com/lh3/seqtk Snakemake https://snakemake.readthedocs.io/en/stable/ scikit-learn https://scikit-learn.org/stable/ taxidTools https://cvua-rrw.github.io/taxidTools/ Tidyverse https://www.tidyverse.org/ VSEARCH https://github.com/torognes/vsearch"},{"location":"benchmark/benchmark/","title":"Benchmark mode","text":"<p>FooDMe offers a benchmarking module that can be used to automatically compare the anaylsis results to a theoritical  sample composition and compute performance indicators.</p> <p>This can be used directly for method validation or verification or to compare the performance of the pipeline with different  parameter sets.</p>"},{"location":"benchmark/benchmark/#benchmarking-parameters","title":"Benchmarking parameters","text":"<p>The benchmarking module requires three additional parameters (see Configuration):</p> <ul> <li><code>reference</code>: Path to a table containing the sample composition information containing the followuing fields:<ul> <li><code>sample</code>: Sample name</li> <li><code>taxid</code>: Taxonomic identifier</li> <li><code>proportion</code>: Expected fraction of the sample made up from this component, in the interval [0, 1]</li> </ul> </li> <li><code>threshold</code>: A minimal quantity for a component to be considered a 'true' result, given in percent.</li> <li><code>target_rank</code>: A maximum taxonomic rank for a component to be considered a 'true' result</li> </ul> <p>Warning</p> <p>Because of limitations of the NCBI taxonomy classification, only Linnaean ranks are supported. This means you can choose only from  the following (case-sensitive) options:</p> <ul> <li>species</li> <li>genus</li> <li>family</li> <li>order</li> <li>class</li> <li>phylum</li> <li>kingdom</li> </ul>"},{"location":"benchmark/benchmark/#usage","title":"Usage","text":"<p>The usage only differs to the basic analysis by pointing snakemake to the <code>benchmark</code> workflow rather than the default <code>Snakefile</code>:</p> <pre><code>conda activate snakemake\nsnakemake -s ~/FooDMe/workflow/benchmark \\\n  --use-conda --conda-prefix ~/conda-envs --cores 1 \\\n  --configfile ~/FooDMe/config/myconfig.yaml\n</code></pre> <p>This will either run the normal analysis followed by the benchmarking module, or just the benchmarking module if the analysis already exists.</p>"},{"location":"benchmark/benchmark/#benchmarking-results","title":"Benchmarking results","text":"<p>The benchmarking results will be saved in the <code>benchmarking</code> subfolder, including  tables and an interactive HTML report. Sample specific metrics will also be saved in each sample's folder, in a specific subfolder.</p>"},{"location":"benchmark/benchmark/#yield","title":"Yield","text":"<p>The yield is given as the amount of reads left in percent of total input reads for each of the three main  analytical steps:</p> <ul> <li>Merged reads correspond to the yield of read pre-processing and merging</li> <li>Clustered reads corresponds to the yield of all steps up to the cluster (OTU or ASV) generation</li> <li>Assigned reads corresponds to the final yield of the analysis</li> </ul> <p>Note</p> <p>For ASV, the merged reads yield includes the denoising steps</p>"},{"location":"benchmark/benchmark/#metrics","title":"Metrics","text":"<p>After filtering the analysis results with the given concentration threshold and maximal rank, the results are compared to a \"true\" composition and several metrics are  calculated to measure the reliability of the analysis.</p> <p>Metrics are calculated for each sample and an aggregated metric is also calculatedthat considers  the whole dataset as a single sample.</p>"},{"location":"benchmark/benchmark/#classification-metrics","title":"Classification metrics","text":"<p>Each result is categorised as either True-positive (TP), False-positive (FP), or False-Negative (FN) depending on wether it is considered true and expected.</p> <p>Note</p> <p>Because the number of true negative is roughly the number of taxid in the database and therefore several orders of magnitude higher than the number of true positives,  we do not calculate metrics reliying on the negative reuslt, such as specificity or ROC.</p> <p>The following metrics are then calculated as:</p> <ul> <li>precision: part of true positives in all results predicted to be true. It is the reciprocal of the False-positve rate </li> </ul> \\[ P=\\frac{TP}{TP+FP} \\] <ul> <li>recall: part of real positives predicted as true. It is the reciprocal of the False-negative rate.</li> </ul> \\[ R=\\frac{TP}{TP+FN} \\] <ul> <li>F1 score: harmonic mean of the precision and recall.</li> </ul> \\[ F1=2\\cdot\\frac{P \\cdot R}{P + R} \\] <ul> <li>Average precision: summarizes the variation of precision and recall across a range of \\(N\\) thresholds    (here concentration thresholds are used). It is an approximation of the area under the precision-recall curve.</li> </ul> \\[ AP=\\sum_{n=1}^{N} (R_n - R_{n-1})P_n \\] <p>Info</p> <p>While the F1 score provides a direct measure of the analysis performance as it ran,  the average precision reflects the overall performance independently of the thrshold.  Combinations of both may be used to determine if the concentration threshold might be optimized.</p>"},{"location":"benchmark/benchmark/#quantification-metrics","title":"Quantification metrics","text":"<p>Two quantification metrics are used here. The 'Distance' metric corresponds to the Euclidian distance of the predicted and true results. It is a reflection of how far away are the results from the  expected composition. We also calculate the 'Mean Absolute Error' as the average value of the difference between predicted and expected concentrations for each samples:</p> \\[ MAE= \\frac{1}{N} \\sum_{n=1}^{N} |expected_n-predicted_n| \\] <p>Note</p> <p>While false negative results account for 0% composition, false positive results  are only indirectly (through their contribution to the total composition) taken into account in the quantification metrics.</p>"},{"location":"benchmark/benchmark/#confusion-matrix","title":"Confusion matrix","text":"<p>The confusion table used to calculate the above mentionned metrics is given as a succint summary  of the results with the following information:</p> <ul> <li><code>Taxid</code>: the expected or determined taxonomic identifier</li> <li><code>match_rank</code>: the rank to which a predicted result was matched to its expected value, given in the Linnaean taxonomical ranks.</li> <li><code>predicted</code>/<code>expected</code>: <code>1</code>or <code>0</code> reprensenting true or false values</li> <li><code>pred_ratio</code>: the predicted amount of this component in the sample</li> <li><code>exp_ratio</code>: the expected amount of this component in the sample</li> </ul>"},{"location":"benchmark/paramspace/","title":"Parameter space exploration","text":"<p>Parameter space exploration allows to easily try different sets of parameters, measure their impact on analysis performance and guide method optimization for new matrices, targets, or databases.</p> <p>Provided a grid of parameter combination, foodme will be run several time with a  different paramter set. Performance metrics will be extracted for each run using using a expected sample composition and the metrics for each run will be aggregated.</p> <p>Importantly you will need to provide a Foodme configuration file (see configuration)  containing default values and a parameter grid containing a set of parameters to update on each row.</p>"},{"location":"benchmark/paramspace/#parameter-grid","title":"Parameter grid","text":"<p>The parameter grid should be organized as a tab-delimited text file where the first row contains the name of the parameters to vary. Each row then contains  a set of parameter that will be used to update the default configuration.</p> <p>For example with the file below, three independent foodme runs will be triggered with ASV clustering, dereplication, and 97% identity clustering. The other parameters will be taken for the default configuration file.</p> cluster_method cluster_identity asv 1 otu 1 otu 0.97"},{"location":"benchmark/paramspace/#configuration","title":"Configuration","text":"<p>The parameter space exploration mode only requires a few argument, that you can either pass to snakemake as a YAML configuration file (see the template in <code>config/</code> or directly through the command line with  the <code>--config</code> argument.</p> Parameter Expected values Description <code>workdir</code> Path Path to the output directory, will be created if it doesn\u00b4t exist <code>foodme_config</code> Path Path to the foodme configuration file. <code>paramspace</code> Path Path to the parameter grid <code>force_rerun</code> True/False Whether to force a recalculation of the foodme results  for all parameter combinations.  Equivalent to the <code>--forceall</code> directive. <p>Note</p> <p>As parameter space exploration only makes sense if you can compare the results  to known sample compositions, it is required to provide values for the <code>benchmark_*</code> parameters. These values can be part of the parameter space exploration too!</p> <p>Warning</p> <p>In the parameter space mode, the foodme configuration file only supports absolute file paths. Relative paths will result in errors, you've been warned.</p>"},{"location":"benchmark/paramspace/#running-a-parameter-space-analysis","title":"Running a parameter space analysis","text":"<p>The parameter space analysis is organized in a separate workflow from foodme, it is therefore nescessary to point snakemake to the correct workflow definition:</p> <pre><code>conda activate snakemake\nsnakemake -s ~/FooDMe/workflow/paramspace \\\n  --use-conda --conda-prefix ~/conda-envs --cores 1 \\\n  --configfile ~/FooDMe/config/myconfig.yaml\n</code></pre> <p>Warning</p> <p>The command <code>--forceall</code> will not force a re-run of foodme. To trigger a rerun you will have to set the <code>force_rerun</code> parameter to <code>True</code>in the configuration or manually delete the <code>foodme_runs</code> folder.</p>"},{"location":"benchmark/paramspace/#results","title":"Results","text":"<p>The parameter space exploration will produce an aggregate of foodme <code>benchmark</code>  results: yields, metrics, PR-curves, and confusion matrices, all with indication of the specific parameter set that was used for each run.</p> <p>Additonally the ressource usage (CPU, memory, I/O) will be measured for each foodme run and saved as a table. See also snakemake's documentation.</p> <p>The output is organized as follows:</p> <pre><code>wordir/\n |- aggregated/             \\\\ Perfomance indicators\n |- benchmark/              \\\\ Ressource usage benchmarking\n |- foodme_runs/            \\\\ Individual foodme runs results \n |   |- param~value/... \n |- logs/                   \\\\ Log files\n</code></pre>"},{"location":"userguide/configuration/","title":"Configuration","text":"<p>Parameters can be freely set to fit specific analytical needs. Different sets of parameters should be saved to configuration files in YAML format. This ensures reproducible analysis of different samples over time.</p> <p>A template of such a configuration file is stored in the repository under <code>FooDME/config/config.yaml</code>.</p> <p>We include an optimized configuration file for use in 16S meat metabarcoding experiments with the  Dobrovolny et al. (2019) method. The <code>config</code> folder will be populated with configuration sets for other matrices when possible. Feel free to submit yours!</p> <p>Warning</p> <p>Path to reference files are system dependent and will still need to be changed even for preset configurations.</p>"},{"location":"userguide/configuration/#how-to-use-to-configuration-file","title":"How to use to configuration file","text":"<p>Modify the values of each parameters as you need (see table below). Then save your own configuration locally (for example under <code>FooDme/config/</code>). This configuration can be reused for successive analysis.</p>"},{"location":"userguide/configuration/#sample-sheet","title":"Sample sheet","text":"<p>The input files must be linked using a sample sheet. A template for such a file  is available under <code>FooDME/config/samples.tsv</code>. and takes the following form:</p> sample fq1 fq2 nameA A_R1.fastq.gz A_R2.fastq.gz nameB B_R1.fastq.gz B_R2.fastq.gz <p>Simply modify the template with your own files  or use the provided script to automatically generate a sample sheet from FASTQ files in a folder:</p> <pre><code>bash ~/FooDMe/ressources/create_sampleSheet.sh --mode illumina --fastxDir ~/raw_data\n</code></pre> <p>This will create a file called <code>samples.tsv</code> in the <code>raw_data</code> folder.</p> <p>Note</p> <p>The above command assumes that FASTQ files are named according to Illumina naming standards. Different naming standards are available. Use <code>--help</code> to see more options.</p> <p>Info</p> <p>This tool was developed by the Federal institute of risk assessment (BfR) in Berlin. More information is available in their repository.</p>"},{"location":"userguide/configuration/#list-of-parameters","title":"List of parameters","text":"Parameter Expected values Description <code>workdir</code> Path Path to the output directory, will be created if it doesn\u00b4t exist <code>samples</code> Path Path to the sample sheet <code>threads_sample</code> Number Number of threads assigned to each job <code>threads</code> Number Number of threads assigned to the workflow <code>primers_fasta</code> Path Path to the fasta file containing primer sequences.IUPAC ambiguous nuclotides are accepted. <code>blast_DB</code> Path Path to the BLAST database in the form <code>path/to/folder/db-name</code> <code>taxdb</code> Path Path to the folder containing the <code>taxdb</code>files <code>rankedlineage_dmp</code> Path Path to the <code>rankedlineage.dmp</code> file from the <code>taxdump</code> archive <code>nodes_dmp</code> Path Path to the <code>nodes.dmp</code> file from the <code>taxdump</code> archive <code>read_length_required</code> Number Minimal length of the reads after primer trimming <code>qualifier-quality-phred</code> Number Minimal quality value per nucleotide <code>qctrim_window-size</code> Number Size of the sliding window for 3\u00b4 quality trimming <code>qctrim_mean_quality</code> Number Minimal quality thresold for sliding average <code>trim_primers_3end</code> True/False Should primers be trimmed on the 3\u00b4 end of the reads? Only relevant if the sequencing length is larger than the amplicon length <code>primer_error_rate</code> Number [0, 1] Maximum error-rate allowed for primer matching <code>amplicon_min_length</code> Number Minimal length of the merge reads or ASV sequences <code>amplicon_max_length</code> Number Maximal lenght of the merge reads or ASV sequences <code>max_expected_errors</code> Number Maximum number of expected errors allowed in merged reads (OTUs) or trimmed reads (ASVs) <code>max_ns</code> Number Maximal number of undetermined <code>N</code> nucleotide per sequence. This will automatically be set to 0 for ASVs <code>cluster_method</code> <code>otu</code> or <code>asv</code> Clustering method <code>cluster_identity</code> Number [0, 1] OTU identity threshold. Only for OTU <code>cluster_minsize</code> Number Minimal size of clusters to keep <code>merging_max_mismatch</code> Number Maximum number of mismatch allowed in the overlap between reads. Only for ASV <code>remove_chimera</code> True/False Should predicted chimeric sequences be removed? <code>min_consensus</code> Number [0.51, 1] Minimal agreement for taxonomic consensus determination. 0.51 is a majority vote and 1.0 is a last common ancestor determination <code>taxid_filter</code> Taxonomic identifier Node under which to perform the BLAST search. Equivalent to pruning the taxonomy above this node. Use the Root Node number to keep the entire taxonomy <code>blocklist</code> <code>extinct</code> or custom path Path to a list of taxonomic identifier to exclude from the BLAST search <code>seq_blocklist</code> <code>None</code> or custom path Path to a list of sequence accessions (e.g. <code>NC_0016400</code>) to exclude from the results <code>blast_filter_low_complexity</code> True/False Wether to mask low-complexity regions in the BLAST search. On by default, deactivate if you expect barcode sequences with low complexity. <code>blast_evalue</code> Number (scientific) Minimal E-value threshold for the BLAST search <code>blast_identity</code> Number [0, 100] Minimal identity (in percent) between the query and hit sequence for the BLAST search <code>blast_qcov</code> Number [0, 100] Percent of the query to be covered by the hit sequence for the BLAST search <code>bit_score_diff</code> Number Maximum bit-score difference between the best and last hit to keep for each query after the BLAST search <code>benchmark_reference</code> Path Path to benchmarking reference table (see Benchmark mode). <code>benchmark_threshold</code> Number [0, 100] Lower limit for benchmarking (see Benchmark mode). <code>benchmark_rank</code> String Highest rank for benchmarking (see Benchmark mode). <p>Note</p> <p>Unless you are running foodme in <code>benchmark</code> mode, you do not need to modify the values of the <code>benchmark_*</code> arguments. See Benchmark mode for more details.</p>"},{"location":"userguide/database/","title":"Nucleotide database","text":"<p>In order to taxonomically assign sequences, a certain amount of reference data is required. Below are the instructions to retrieve standard databases and how to create custom ones.</p>"},{"location":"userguide/database/#blast-database","title":"BLAST database","text":"<p>The sequence comparison step as implemented uses the BLAST command line tools. This requires that the  nucleotide database is indexed and formatted in a BLAST-specific way.</p>"},{"location":"userguide/database/#preformatted-databases","title":"Preformatted databases","text":"<p>A large pre-formatted nucleotide sequence database is freely available from the NCBI servers. The BLAST NT database contains a collection of sequences from different sources and can be downloaded directly from the NCBI servers. </p> <p>A script to fetch the BLAST NT database and additional required taxonomic definitions  is available with FooDMe:</p> <pre><code>cd ~\nmkdir blast_nt\nbash ~/FooDMe/ressources/fetch_nt_blast.sh -d blast-nt\n</code></pre> <p>Running the above commands will create the blast_nt directory and retrieve all the  nescessary files from the NCBI servers.</p> <p>Warning</p> <p>Downloading the BLAST NT database will require a large chunk of available memory and take several hours. </p>"},{"location":"userguide/database/#custom-databases","title":"Custom databases","text":"<p>A collection of non-redundant reference sequences (RefSeq) is also available from the NCBI servers. The collection is not yet in a BLAST format but we provide a script to retrieve and format it.</p> <p>Running the commands below will create a folder called refseq, download the RefSeq collection and  format it in a blast format. This requires to create a conda environment containing the blast tools.</p> <pre><code>mamba create -n blast -c conda-forge -c bioconda blast\nconda activate blast\ncd ~\nmkdir refseq\nbash ~/FooDMe/ressources/create_RefSeq_blastdb.sh -d refseq -t\n</code></pre> <p>Additionally it is possible to format any sequence collection in a BLAST compliant format. A User Guide therefore is available from the BLAST documentation.</p>"},{"location":"userguide/database/#additonal-files","title":"Additonal files","text":"<p>In addition to the nucleotide collection, several taxonomy definition files are nescessary. These are available from the NCBI servers and can be used with other sources.</p> <p>Warning</p> <p>The links below are provided as information and might not link to the most recent  version of the files. </p> <p>Note</p> <p>If using the scripts above to retrieve either the NT or RefSeq database, the files  below should already be included in your local database.</p>"},{"location":"userguide/database/#taxonomic-information","title":"Taxonomic information","text":"<p>The <code>taxdb</code> files are nescessary to link taxonomic identifier (taxid) and human-readable  informations. These can be downloaded here:</p> <p>https://ftp.ncbi.nlm.nih.gov/blast/db/taxdb.tar.gz</p>"},{"location":"userguide/database/#taxonomic-classification","title":"Taxonomic classification","text":"<p>The <code>taxdump</code> files contain the taxonomy hierarchical information and are nescessary  to determine the degree of relationship between different taxa. They can be downloaded here:</p> <p>https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump.tar.gz</p>"},{"location":"userguide/installation/","title":"Installation","text":"<p>Below is a detailled installation guide. Beginners are encouraged to follow this guide step by step. </p> <p>Note</p> <p>The examples below will describe an installation in the <code>/home/username</code>. It is assumed that raw sequencing data are stored in the <code>raw_data</code> folder in the user\u00b4s home directory.</p>"},{"location":"userguide/installation/#requirements","title":"Requirements","text":"<p>Because some of the tools used by the workflow only work in a UNIX environment, Windows OS is not supported.</p> <p>The ressources needed to run the pipeline vary according to the nucleotide database used. In general be sure to have enough hard-drive space to store the databases and the raw data plus  space to store the results. </p> <p>The workflow will remove tempory intermediate files on the fly to minimize the memory footprint.</p> <p>Note</p> <p>Depending on the analysis mehtod, sequencing depth, and samples complexity, the persistent output should be below 100MB per sample.</p> <p>Additionally the BLAST step will require an amount of virtual memory equivalent to the size of  the database. This can be either hard drive space or RAM.</p> <p>Increasing the number of cores will considerably speed up the workflow by taking advantage of  parallelization.</p> <p>Info</p> <p>Minimal configuration:</p> <ul> <li>500 Gb Hard drive space </li> <li>8 Gb RAM</li> </ul> <p>The workflow can therefore run on a medium range laptop, even within a Virtual Machine emulating Linux. </p> <p>An internet connection will be nescessary for the first run of the pipeline. Successive runs can be  performed without an internet connection</p>"},{"location":"userguide/installation/#conda","title":"Conda","text":"<p>Snakemake makes intensive use of the environment manager conda. There are many different distributions of conda to choose from, each with their advantages or inconvenients. For a new installation we recommend using the minimalistic distribution miniconda.</p> <p>For an installation guide, see the Bioconda documentation, specifically steps 1 and 2:</p> <p>Quote</p> <p>1- Install conda</p> <p>Bioconda requires the conda package manager to be installed. If you have an Anaconda Python installation, you already have it. Otherwise, the best way to install it is with the Miniconda package. The Python 3 version is recommended.</p> <p>On MacOS, run:</p> <pre><code>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\nsh Miniconda3-latest-MacOSX-x86_64.sh\n</code></pre> <p>On Linux, run:</p> <p>curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh sh Miniconda3-latest-Linux-x86_64.sh</p> <p>Follow the instructions in the installer. If you encounter problems, refer to the Miniconda documentation. You can also join our Gitter channel to ask other users for help.</p> <p>2- Set up channels</p> <p>After installing conda you will need to add the bioconda channel as well as the other channels bioconda depends on. It is important to add them in this order so that the priority is set correctly (that is, conda-forge is highest priority).</p> <p>The conda-forge channel contains many general-purpose packages not already found in the defaults channel.</p> <pre><code>conda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n</code></pre> <p>The dependency solver of conda being notoriously slow and helpless in front  of complex environments, it is advised to supplement the conda installation  with a better solver called mamba:</p> <pre><code>conda install -n base -c conda-forge mamba\n</code></pre>"},{"location":"userguide/installation/#download-the-repository","title":"Download the repository","text":"<p>Install git:</p> <pre><code>mamba install -n base -c conda-forge git\n</code></pre> <p>Then get a copy of the workflow:</p> <pre><code>cd ~ \ngit clone https://github.com/CVUA-RRW/FooDMe.git\n</code></pre> <p>Info</p> <p>Without git, you can download the repository manually and unpack the archive locally.</p> <p>Having git installed will later allow you to get the latest version of the workflow as well keep the older versions archived for reproducibility and traceability purposes.</p> <p>For example update the workflow with:</p> <pre><code>cd ~/FooDMe\ngit pull origin\n</code></pre>"},{"location":"userguide/installation/#set-up-a-conda-environment-to-run-the-workflow","title":"Set up a conda environment to run the workflow","text":"<p>Running the workflow will require snakemake to be installed in the current environment. To avoid future conflicts between software versions, it is recommended to create  a new environment to execute snakemake:</p> <pre><code>mamba create -n snakemake -c conda-forge -c bioconda snakemake\n</code></pre> <p>The snakemake environment can then be toggled on and off with:</p> <pre><code>conda activate snakemake\nconda deactivate\n</code></pre>"},{"location":"userguide/installation/#test-the-installation","title":"Test the installation","text":"<p>The repository comes with a minimal dataset allowing to run a quick test of the installation. Running this example is also a good occasion to initialize all the software dependencies  that will be reused on later runs. </p> <p>In this examples we will store the workflow\u00b4s enviroments under <code>~/conda-envs</code>.</p> <pre><code>cd ~/FooDMe\nconda activate snakemake\nsnakemake --use-conda --conda-prefix ~/conda-envs --cores 1\n</code></pre> <p>Snakemake will the start creating the nescessary conda environments (this can take a few minutes) before analyzing the three examples samples in the <code>.tests</code> folder.</p> <p>Note</p> <p>The created conda environment can be reused in later runs by specifying the argument <code>--conda-prefix ~/conda-envs</code> when executing snakemake. This will save the environment  creation time.</p> <p>Warning</p> <p>The <code>.tests</code> folder might be hidden on your file explorer. If you don\u00b4t see it, enable 'view hidden files' in the options.</p> <p>Feel free to explore the files produced by this first run in the <code>.tests</code> folder.  More details about the output and the use of snakemake will be given in the later sections of this guide.</p>"},{"location":"userguide/overview/","title":"Overview","text":"<p>Welcome to the User Manual of FooDMe. This documentation will guide you through your first steps with the pipeline.</p>"},{"location":"userguide/overview/#aim","title":"Aim","text":"<p>The aim of this workflow is the taxonomic assignement of paired-end Illumina amplicon sequencing reads. It was developped with a focus of food and feed authenticity control.</p>"},{"location":"userguide/overview/#workflow-description","title":"Workflow description","text":"<p>The workflow consists of three main steps that will be performed sequentially: reads pre-processing, clustering, and taxonomic assignment.</p>"},{"location":"userguide/overview/#input-data","title":"Input data","text":"<p>The pipeline expects paired-end Illumina reads, provided as paths in a tabular file. See the configuration help for more details.</p>"},{"location":"userguide/overview/#pre-processing","title":"Pre-processing","text":"<p>As a first analysis step, primers will be trimmed from the reads. By default primers are only matched on  the 5' end of the reads. In some cases (e.g. Sequencing is longer than the amplicon length) one may want to trim primers on the 3' end as well. This behaviour is supported and can be enabled in the parameters. </p> <p>The reads will then be pre-processed for quality trimming on the 3' end using a sliding window checking for minimal  quality requirements.</p>"},{"location":"userguide/overview/#clustering","title":"Clustering","text":"<p>FooDMe implements three different clustering strategies to choose from. Each has specific performances that can be better suited to your specific needs.</p>"},{"location":"userguide/overview/#identity-clustering","title":"Identity clustering","text":"<p>For this strategy, sequences are first dereplicated and ranked by abundance. Each sequence is then compaired to an itinially empty list of centroids and,  based on sequence similarity, is either assigned as a new centroid or merged with  the most similar existing centroid.</p> <p>The degree of identity required for clustering the sequences can be freely set, 0.97  being a commonly accepted value and 1.0 corresponding to a dereplication.</p> <p>This approach is very good at smoothing out seqencing noise but can also result in the clustering of highly similar sequences for different organisms.</p>"},{"location":"userguide/overview/#dereplication","title":"Dereplication","text":"<p>With this strategy, only identical reads will be clustered together. This is effectively implemented as an identity clustering strategy with 100% identity. This is more suitable for high sensitivity identification of amplicons but the sequencing noise will not be filtered.</p> <p>Due to the large number of clusters resulting from this strategy this is also the most  computationally expensive of the three.</p>"},{"location":"userguide/overview/#denoising","title":"Denoising","text":"<p>For the denoising strategy, the nucleotide substitution rate of the sequencing run  will be modelled based on the available data. Using this model, reads can be corrected  for sequencing-induced substitutions. This results in a typically low number of cluster,  close to the biological reality of the sample.</p> <p>Note</p> <p>Clusters are often refered to as Operational Taxonomic Units (OTUs) for dereplication and identity clustering and as Amplicon Sequence Variants (ASVs) for denoising.</p>"},{"location":"userguide/overview/#taxonomic-assignment","title":"Taxonomic assignment","text":""},{"location":"userguide/overview/#blast-search-and-filtering","title":"BLAST search and filtering","text":"<p>The representative sequences for each cluster are compaired to a user-provided nucleotide database using Basic Local Alignment Search (BLAST) and references satisfying specified  similarity critera are recovered.</p> <p>In most cases, only part of the database is relevant for each application. For this purpose it is  possible to specify a taxonomic node (for example Vertebrates) to which to limit the BLAST search.</p> <p>Specific taxa can also be irrelevant, such as common contaminants or extinct species. This taxa can be provided as a list of identifiers in a text file and will be filtered out of the BLAST search.</p> <p>Info</p> <p>We use the NCBI Taxonomy nomenclature, including lineages and identifiers. See https://www.ncbi.nlm.nih.gov/taxonomy for more details.</p> <p>Because this typically results in a large number of matching results (and taxa), the matches  can be post-filtered based on their alignment quality as measured by the alignement bit-scores.</p>"},{"location":"userguide/overview/#taxonomic-consensus","title":"Taxonomic consensus","text":"<p>As this process often results in a unclear mix of taxa, a consensus can be determined based  on the underlying taxonomic hierachy and a minimal agreement level that can be freely set between strict majority (0.51) and last-common ancestor (1.0).</p>"},{"location":"userguide/overview/#ouput","title":"Ouput","text":"<p>Statistics on the results of each processing steps as well as the final results with the  composition of each samples are saved in tabular files and in an HTML report whose tables can be exported to popular formats such as Excel or PDF. More info here.</p>"},{"location":"userguide/overview/#workflow-chart","title":"Workflow chart","text":"<p>Below is a schematical overview of the data processing:</p> <pre><code>flowchart TD\n    A[Raw data] --&gt; B(Primer trimming)\n    subgraph Pre-processing\n    B ---&gt; C(Quality trimming)\n    end\n    C ---&gt;|OTU| D1(Read merging)\n    C ---&gt;|ASV| D2(Denoising)\n    subgraph Clustering\n    D1 ---&gt; E1(Cluster filtering)\n    D2  ---&gt; E2(Read merging)\n    end\n    Z1(Branch filter) ---&gt; Z0[(BLAST database)]\n    Z2(Blocklist) ---&gt; Z0\n    Z0 ---&gt; F(Local Alignment search)\n    E1 ---&gt; F\n    E2 ---&gt; F\n    Y[(Taxonomy definitions)] ---&gt; H(Consensus determination)\n    subgraph Taxonomic assignement\n    F ---&gt; J(Accession filtering)\n    J ---&gt; G(Bitscore filtering)\n    G ---&gt; H\n    end\n    H ---&gt; I[Report]</code></pre>"},{"location":"userguide/results/","title":"Viewing the results","text":"<p>The analysis produces a number of files containing quality reports, tables and graphs,  as well as some of the processed data at different steps of the analysis.</p> <p>Note</p> <p>By default, large files such as trimmed FASTQ are removed during the analysis use snakemake\u00b4s <code>--notemp</code> to keep all files.</p>"},{"location":"userguide/results/#folder-structure","title":"Folder structure","text":"<p>The output folder follows the following structure:</p> <pre><code>workdir/\n|- common/             \\\\ Taxonomy processing files\n|- logs/\n|   |- all/            \\\\ Logs for steps processing aggregated results\n|   |- common/         \\\\ Logs for taxonomy produces\n|   |- sample_name/    \\\\ Sample-wise log files\n|- reports/            \\\\ All analysis reports\n|- sample_name/        \\\\ Sample-wise processed data are located\n    |- denoising/      \\\\    here, splitted in the relevant folders\n    |- krona/\n    |- taxonomy\n    |- trimmmed\n    |- reports         \\\\ Analysis reports for individual samples\n</code></pre>"},{"location":"userguide/results/#report","title":"Report","text":"<p>All the reports are stored as tab-delimited text files in the <code>reports</code> folder. Additionaly an HTML report is produced that contains the information of the individual files. Tables in this report are interactive and can be filtered, columns moved or removed, and  the tables can be exported in excel, csv, and pdf formats.</p>"},{"location":"userguide/results/#panel-descriptions","title":"Panel descriptions","text":"<p>The HTML report is divided in several sections reflecting the different processing steps.</p> <p>Tables can be sorted using the arrows in each header cell, filtered using the fields below the header, columns can be moved by drag and drop, and column visibility toggled using the buttoin above the table. Additional buttons above the table allow to copy the entire table to the clipboard, print it, or export it  in popular formats such as Excel or PDF.</p>"},{"location":"userguide/results/#quality-summary","title":"Quality summary","text":"<p>This presents a succint summary of the most important quality statistics of the analyis.</p> <ul> <li>Sample: sample identifier </li> <li>Q30 rate: proportion of bases above Q30 after quality processing</li> <li>Insert size peak: Estimated mean size of the insert</li> <li>Read number: total input reads</li> <li>Pseudo-reads: number of merged reads</li> <li>Reads in ASV/OTU: number of clustered reads</li> <li>ASV/OTU number: number of clusters</li> <li>Assigned reads: number of reads assigned to a taxa</li> <li>Rank-consensus: number of clusters with a consensus at the given rank</li> <li>No match: number of clusters that could not be successfully assigned</li> </ul>"},{"location":"userguide/results/#trimming-statistics","title":"Trimming statistics","text":"<p>Summary of the primer and quality trimming processes.</p> <ul> <li>Sample: sample identifier</li> <li>Total raw reads: number of inputs reads (forward + reverse)</li> <li>Total reads after primer trimming: number of reads \u00b4left after primer trimming. Reads where primers cannot be found are discarded.</li> <li>No primer found: percent of reads where primers could not be found</li> <li>Total bases before quality trim: number of nucleotide in the input reads</li> <li>Total reads after qualtiy trim: number of pre-processed reads available for clustering</li> <li>Total bases after quality trim: number of nucleotide after the pre-processing</li> <li>Q20 after: propoertion of bases above Q20 after quality processing</li> <li>Q30 rate: proportion of bases above Q30 after quality processing</li> <li>Duplication rate: number of duplicated reads</li> <li>Insert size peak: Estimated mean size of the insert</li> <li>links: Link to the fastp report</li> </ul>"},{"location":"userguide/results/#read-filtering-statistics-otu-only","title":"Read filtering statistics (OTU only)","text":"<p>Summary of the quality controls of reads prior to clustering.</p> <ul> <li>Sample: sample identifier</li> <li>Total reads: number of input reads for the error-correction process</li> <li>Pseudo-reads:  number of reads resulting form merging process</li> <li>Merging failure: proportion of reads that could not be merged</li> <li>Pseudo-reads PF: number of merged-reads passing size filters</li> <li>Discarded reads: proportion of merged-reads not passing size filter</li> <li>Unique sequences: Number and proportion of distinct sequences</li> </ul>"},{"location":"userguide/results/#clustering-statistics-otu-only","title":"Clustering statistics (OTU only)","text":"<p>Summary of the OTU clustering process.</p> <ul> <li>Sample: sample identifier</li> <li>Unique sequences: Number and proportion of distinct sequences</li> <li>Clusters: Number of clusters passing minimal population requirement</li> <li>Discarded clusters: proportion of reads and clusters not passing minimal population requirement</li> <li>Non-chimeric clusters: number of cluster not flagged as chimeric</li> <li>Chimeras: proportion of clusters and reads flagged as chimeric and removed</li> <li>Pseudo-reads clustered: number ands proportion of pseudo-reads successfully clustered</li> </ul>"},{"location":"userguide/results/#denoising-statistics-asv-only","title":"Denoising statistics (ASV only)","text":"<p>Summary of the denoising and read merging steps.</p> <ul> <li>Sample: sample identifier</li> <li>Total reads: number of input reads for the error-correction process</li> <li>Filtered reads: number of reads passing size filters</li> <li>Discarded reads: proportion of reads not passing size filter</li> <li>Denoised R1/2: number of successfully corrected reads in the respective orientation</li> <li>Merged: number of reads resulting form merging process</li> <li>Merging failure: proportion of reads that could not be merged</li> <li>ASV: number of clustered</li> <li>Size-filtered ASV: number of clusters passing minimal population requirements</li> <li>Discarded ASVs: proportion of clusters and reads not passing size requirements</li> <li>Non-chimeric ASV: number of cluster not flagged as chimeric</li> <li>Chimeras: proportion of clusters and reads flagged as chimeric and removed</li> <li>Reads in ASVs: final number and proportion of reads succesfully clustered</li> </ul>"},{"location":"userguide/results/#taxonomic-assignment","title":"Taxonomic assignment","text":"<p>Summary of the taxonomic assignment of individual cluster.</p> <ul> <li>Sample: sample identifier</li> <li>Query: cluster identifier</li> <li>Count: cluster population</li> <li>Blast hits: number of BLAST results for this cluster</li> <li>Best bit-score: sequence alignment score of the best BLAST result</li> <li>Lowest bit-score: sequence alignment score of the worst BLAST result</li> <li>Bit-score threshold: minimal sequence alignment score for result to be accepted</li> <li>Saved Blast hits: number of sequences passing the sequence alignment threshold</li> <li>Consensus: scientific name of the determined taxonomic consensus</li> <li>Rank: rank of the taxonomic consensus</li> <li>Taxid: taxonomic identifier of the consensus</li> <li>Disambiguation: scientific names and proportions of all saved Blast hits used for taxonomic consensus</li> <li>blast_report: link to raw BLAST results</li> <li>filtered report: link to BLAST report after alignment quality filtering</li> </ul>"},{"location":"userguide/results/#taxonomic-assignment-statistics","title":"Taxonomic assignment statistics","text":"<p>Sample-wise summarized assignemnt report.</p> <ul> <li>Sample: sample identifier</li> <li>Query: number of clusters</li> <li>Unknown sequences: number and proportion of sequnces that did not yield BLAST results</li> <li>Rank consensus: Number and proportion of clusters assigned at the given rank</li> </ul>"},{"location":"userguide/results/#metabarcoding-results","title":"Metabarcoding results","text":"<p>Sample composition summary.</p> <ul> <li>Sample: sample identifier</li> <li>Consensus: scientific name of the determined taxonomic consensus</li> <li>Rank: rank of the taxonomic consensus</li> <li>Taxid: taxonomic identifier of the consensus</li> <li>Count: number of reads assigned to the taxon</li> <li>Disambiguation: scientific names and proportions of all saved Blast hits used for taxonomic consensus</li> <li>Percent of total: proportion of reads assigned to this taxon</li> <li>Percent of assigned: proportion of the assigned reads (excluding those with no consensus) assigned to this taxon</li> </ul>"},{"location":"userguide/results/#graphical-results","title":"Graphical results","text":"<p>A graphical overview of the sample compositions.  Several options can be changed and a snapshot downloaded.</p>"},{"location":"userguide/results/#versions","title":"Versions","text":"<p>Summary of the tools and databases used and their version or modification dates.</p>"},{"location":"userguide/run/","title":"Starting a run","text":"<p>Now that the configuration and sample sheets are ready, we can start an  analysis!</p>"},{"location":"userguide/run/#basic-usage","title":"Basic usage","text":"<p>Starting and controlling the workflow execution is done through the <code>snakemake</code> command.</p> <pre><code>conda activate snakemake\nsnakemake --use-conda --conda-prefix ~/conda-envs --cores 1 \\\n  --configfile ~/FooDMe/config/myconfig.yaml\n</code></pre> <p>Info</p> <p>Depending on your system you may want to assign more cores to the analysis.  Each additional core will allow running another job in parallel, considerably speeding up execution time. You can adjust the number of assiged cores with <code>--cores N</code> whereby N is the number of cores.</p> <p>Info</p> <p>Specifying a prefix for the conda environment is not nescessary but will allow you to reuse created environments  between runs. Doing so will save up, time, ressources, and memory on each execution.</p>"},{"location":"userguide/run/#run-specific-parameters","title":"Run-specific parameters","text":"<p>As creating or modifying the configuration file to change the sample sheet and the output  directory for each run can be a bit cumbersome, it is possible to dynamically modify  parameters at execution time. </p> <pre><code>WORKDIR=$(date +%F)_foodme\nSAMPLES=~/raw_data/samples.tsv\n\nsnakemake --use-conda --conda-prefix ~/conda-envs --cores 1 \\\n  --configfile ~/FooDMe/config/myconfig.yaml \\\n  --config workdir=${WORKDIR} samples=${SAMPLES}\n</code></pre> <p>The above example will use the sample sheet located under <code>~/raw_data/samples.tsv</code> and outputs the analysis results in the <code>[YYYY-MM-DD]_foodme</code> folder, regardless  of the value for the <code>workdir</code> and <code>samples</code> arguments in the config file. The rest of the parameters will be taken form the configuration as usual.</p>"},{"location":"userguide/run/#advanced-snakemake-options","title":"Advanced Snakemake options","text":"<p>A large number of options are available when running snakemake that be can  seen using the <code>--help</code> argument or in the online documentation.</p> <p>A few notables commands are:</p> <ul> <li><code>--forceall</code> will force the recalculation of all the workflow steps. Usefull to repeat an analysis when something went wrong.</li> <li><code>--notemp</code> will disable temporary file removal. This will considerably increase the size of the output but can be useful to troubleshoot an analysis or puzzling results.</li> <li><code>--report</code> will output run statistics.</li> </ul>"}]}